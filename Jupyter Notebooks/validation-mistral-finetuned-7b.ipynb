{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzsBIDk_35OJ"
   },
   "outputs": [],
   "source": [
    "!pip install opik\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth\n",
    "!pip install torchinfo\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jj6KpBy54u0_"
   },
   "outputs": [],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3nOBRUa4yUA"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "# message history part remains\n",
    "import re\n",
    "import os\n",
    "# is cuda version error persists, with incompatibility issue, try this !!\n",
    "# os.environ[\"BNB_CUDA_VERSION\"] = \"117\"\n",
    "# print(os.environ)\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "# Set environment variable to control model cache location (optional)\n",
    "# Uncomment and modify the path if you want to change the default cache location\n",
    "# os.environ[\"HF_HOME\"] = \"/path/to/your/model/cache\"\n",
    "\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"sayantanBiswas/mistral-7b-v0.3\"  # Change this to your preferred model\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "class ChatModel:\n",
    "    def __init__(self, model_id):\n",
    "        print(f\"Loading model {model_id} on {DEVICE} with {DTYPE}...\")\n",
    "\n",
    "        # Load the model with Unsloth\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_id,\n",
    "            max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dtype=DTYPE\n",
    "        )\n",
    "\n",
    "        # Configure generation parameters\n",
    "        self.generation_config = {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"do_sample\": True,\n",
    "            \"use_cache\": True,\n",
    "        }\n",
    "\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    def generate_response(self, history):\n",
    "        # Format the chat history for the model\n",
    "        # formatted_prompt = self.format_chat_history(history)\n",
    "        # print('model input: ', history)\n",
    "        original = ''\n",
    "        # Extract Response sections independently\n",
    "        response_pattern = r'### Response:\\s+(.*?)(?=###|\\Z)'\n",
    "        response_matches = re.findall(response_pattern, history, re.DOTALL)\n",
    "\n",
    "        # Clean and store response matches\n",
    "        if response_matches:\n",
    "            cleaned_responses = [match.strip() for match in response_matches if match.strip()]\n",
    "            if cleaned_responses:\n",
    "                original = cleaned_responses\n",
    "\n",
    "\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer([history], return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate without streaming\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                **self.generation_config\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        generated_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Return the full response\n",
    "        return original[0], generated_response\n",
    "\n",
    "    # def extract_input_response(self, text):\n",
    "    #     results = {}\n",
    "\n",
    "    #     # Extract Input sections independently\n",
    "    #     input_pattern = r'### Input:\\s+(.*?)(?=###|\\Z)'\n",
    "    #     input_matches = re.findall(input_pattern, text, re.DOTALL)\n",
    "\n",
    "    #     # Clean and store input matches\n",
    "    #     if input_matches:\n",
    "    #         cleaned_inputs = [match.strip() for match in input_matches if match.strip()]\n",
    "    #         if cleaned_inputs:\n",
    "    #             results['Input'] = cleaned_inputs\n",
    "\n",
    "    #     # Extract Response sections independently\n",
    "    #     response_pattern = r'### Response:\\s+(.*?)(?=###|\\Z)'\n",
    "    #     response_matches = re.findall(response_pattern, text, re.DOTALL)\n",
    "\n",
    "    #     # Clean and store response matches\n",
    "    #     if response_matches:\n",
    "    #         cleaned_responses = [match.strip() for match in response_matches if match.strip()]\n",
    "    #         if cleaned_responses:\n",
    "    #             results['Response'] = cleaned_responses\n",
    "\n",
    "    #     # If nothing was found, return None\n",
    "    #     if not results:\n",
    "    #         return None\n",
    "\n",
    "    #     return results\n",
    "\n",
    "    # def format_chat_history(self, history):\n",
    "    #     with open('history.txt', 'a') as f:\n",
    "    #         f.write(str(history)+'\\n'+'*'*10+'\\n')\n",
    "    #     \"\"\"Format prompt using only the last turn, optionally with one prior turn for context.\"\"\"\n",
    "\n",
    "    #     alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    #     ### Instruction:\n",
    "    #     {}\n",
    "\n",
    "    #     ### Input:\n",
    "    #     {}\n",
    "\n",
    "    #     ### Response:\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     # Initialize variables to handle previous user and assistant messages\n",
    "    #     prev_user = \"\"\n",
    "    #     prev_assistant = \"\"\n",
    "    #     context = \"\"\n",
    "\n",
    "    #     # Include one previous exchange as context (optional)\n",
    "    #     if len(history) > 1:\n",
    "    #         print('history: ', history, len(history))\n",
    "\n",
    "    #         # Get the previous exchange\n",
    "    #         prev_exchange = history[-2][1]\n",
    "\n",
    "    #         # Extract input and response from previous exchange\n",
    "    #         results = self.extract_input_response(prev_exchange)\n",
    "\n",
    "    #         if results is not None:\n",
    "    #             prev_user_list = results.get('Input')\n",
    "    #             prev_assistant_list = results.get('Response')\n",
    "\n",
    "    #             prev_user = '\\n'.join(prev_user_list) if prev_user_list else ''\n",
    "    #             prev_assistant = '\\n'.join(prev_assistant_list) if prev_assistant_list else ''\n",
    "\n",
    "    #             # Create context with previous exchange\n",
    "    #             context = f\"\\n{prev_user}\\n\\n{prev_assistant}\\n\"\n",
    "\n",
    "    #     current_user_input = history[-1][0]\n",
    "    #     full_input = context + f\"\\n{current_user_input}\"\n",
    "\n",
    "    #     return alpaca_prompt.format(\n",
    "    #         \"Answer the user's question accurately, thoroughly, and helpfully. Provide clear explanations with relevant details. If asked about medical or health-related topics, give informative responses while maintaining a balanced and educational tone.\",\n",
    "    #         full_input\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZOMDDJ05S2M"
   },
   "outputs": [],
   "source": [
    "\n",
    "chat = ChatModel(MODEL_ID)\n",
    "tokenizer = chat.tokenizer\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    # instructions = examples[\"instruction\"]\n",
    "    # inputs       = examples[\"input\"]\n",
    "    # outputs      = examples[\"output\"]\n",
    "\n",
    "    # print(examples)\n",
    "\n",
    "    instructions = \"Answer the user's question accurately, thoroughly, and helpfully. Provide clear explanations with relevant details. If asked about medical or health-related topics, give informative responses while maintaining a balanced and educational tone.\"\n",
    "\n",
    "    inputs = examples['text'].split('<HUMAN>:')[-1].split('<ASSISTANT>:')[0].strip()\n",
    "    outputs = examples['text'].split('<HUMAN>:')[-1].split('<ASSISTANT>:')[1].strip()\n",
    "\n",
    "    # print(inputs)\n",
    "    texts = []\n",
    "    # for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "    text = alpaca_prompt.format(instructions, inputs, outputs) + EOS_TOKEN\n",
    "        # texts.append(text)\n",
    "    return { \"text\" : text, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"heliosbrahma/mental_health_chatbot_dataset\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JSR1_fslAMg"
   },
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_5wndot9BIV"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "val_data = []\n",
    "for i in trange(len(dataset)):\n",
    "\n",
    "  original, response = chat.generate_response(dataset[i]['text'])\n",
    "\n",
    "  # Extract only the response part using regex\n",
    "  pattern = r\"### Response:(.*?)(?=###|\\Z)\"\n",
    "  match = re.search(pattern, response, re.DOTALL)\n",
    "\n",
    "  if match:\n",
    "      clean_response = match.group(1).strip()\n",
    "  else:\n",
    "      clean_response = \"Failed to extract response\"\n",
    "\n",
    "  val_data.append((clean_response,original))\n",
    "\n",
    "  # print(response)\n",
    "  # print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYgRPJuK5ut2"
   },
   "outputs": [],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sJenuJ88vO-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDSGqcKDvAgX"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple whitespace + punctuation tokenizer.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text.lower().split()\n",
    "\n",
    "def calculate_bleu_simple(reference, candidate, max_n=4):\n",
    "    \"\"\"\n",
    "    Simplified BLEU score (no external dependencies).\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    for n in range(1, min(max_n + 1, len(candidate) + 1)):\n",
    "        candidate_ngrams = [tuple(candidate[i:i+n]) for i in range(len(candidate) - n + 1)]\n",
    "        candidate_counts = Counter(candidate_ngrams)\n",
    "\n",
    "        max_counts = Counter()\n",
    "        for ref in reference:\n",
    "            ref_ngrams = [tuple(ref[i:i+n]) for i in range(len(ref) - n + 1)]\n",
    "            ref_counts = Counter(ref_ngrams)\n",
    "            for ngram, count in ref_counts.items():\n",
    "                max_counts[ngram] = max(max_counts[ngram], count)\n",
    "\n",
    "        clipped = {ngram: min(count, max_counts[ngram]) for ngram, count in candidate_counts.items()}\n",
    "        numerator = sum(clipped.values())\n",
    "        denominator = sum(candidate_counts.values())\n",
    "        precision = numerator / denominator if denominator > 0 else 0\n",
    "        precisions.append(precision)\n",
    "\n",
    "    # Brevity penalty\n",
    "    ref_lens = [len(ref) for ref in reference]\n",
    "    closest_ref_len = min(ref_lens, key=lambda x: abs(x - len(candidate)))\n",
    "    bp = math.exp(1 - closest_ref_len / len(candidate)) if len(candidate) < closest_ref_len else 1.0\n",
    "\n",
    "    if all(p > 0 for p in precisions):\n",
    "        bleu = bp * math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
    "    else:\n",
    "        bleu = 0\n",
    "    return bleu\n",
    "\n",
    "def calculate_sacrebleu(reference, hypothesis):\n",
    "    return sacrebleu.sentence_bleu(hypothesis, [reference]).score\n",
    "\n",
    "def calculate_perplexity_4gram(text1, text2):\n",
    "    \"\"\"4-gram Laplace-smoothed perplexity between machine-generated (text1) and original (text2).\"\"\"\n",
    "    def tokenize(text):\n",
    "        text = text.replace('</s>', '')\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "\n",
    "    def build_ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "    tokens1 = tokenize(text1)\n",
    "    tokens2 = tokenize(text2)\n",
    "\n",
    "    fourgram_counts = Counter(build_ngrams(tokens2, 4))\n",
    "    trigram_counts = Counter(build_ngrams(tokens2, 3))\n",
    "\n",
    "    vocab = set(tokens1 + tokens2)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    test_fourgrams = build_ngrams(tokens1, 4)\n",
    "    if not test_fourgrams:\n",
    "        return float('inf')  # Prevent division by zero\n",
    "\n",
    "    log_sum = 0\n",
    "    for fg in test_fourgrams:\n",
    "        prefix = fg[:-1]\n",
    "        prob = (fourgram_counts.get(fg, 0) + 1) / (trigram_counts.get(prefix, 0) + vocab_size)\n",
    "        log_sum += math.log2(prob)\n",
    "\n",
    "    avg_log_prob = log_sum / len(test_fourgrams)\n",
    "    perplexity = math.pow(2, -avg_log_prob)\n",
    "    return perplexity\n",
    "\n",
    "def calculate_jaccard_similarity(text1, text2):\n",
    "    tokens1 = set(re.findall(r'\\w+|[^\\w\\s]', text1.lower()))\n",
    "    tokens2 = set(re.findall(r'\\w+|[^\\w\\s]', text2.lower()))\n",
    "    intersection = len(tokens1 & tokens2)\n",
    "    union = len(tokens1 | tokens2)\n",
    "    return intersection / union if union > 0 else 1.0\n",
    "\n",
    "def score_calculator(machine_generated, original):\n",
    "    machine_tokens = simple_tokenize(machine_generated)\n",
    "    original_tokens = simple_tokenize(original)\n",
    "    reference = [original_tokens]\n",
    "    candidate = machine_tokens\n",
    "\n",
    "    bleu_score = calculate_bleu_simple(reference, candidate)\n",
    "    sacre_score = calculate_sacrebleu(original, machine_generated)\n",
    "    perplexity_score = calculate_perplexity_4gram(machine_generated, original)\n",
    "    jaccard_score = calculate_jaccard_similarity(machine_generated, original)\n",
    "\n",
    "    return bleu_score, sacre_score, perplexity_score, jaccard_score\n",
    "\n",
    "\n",
    "scores = []\n",
    "for machine, original in tqdm(val_data):\n",
    "    bleu, sacre, perplexity, jaccard = score_calculator(machine, original)\n",
    "    scores.append((bleu, sacre, perplexity, jaccard))\n",
    "\n",
    "# scores\n",
    "import numpy as np\n",
    "np.mean(scores, axis = 0)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNHbaFltVElk9PwLctYA9nf",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
