{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_S0Bnmzx_S-"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4nWp_bQx_S-"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "    !pip install torchinfo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0Vz0fHTx_S-"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uv4tcaqpcWyB"
   },
   "outputs": [],
   "source": [
    "\n",
    "# is cuda version error persists, with incompatibility issue, try this !!\n",
    "# import os\n",
    "# os.environ[\"BNB_CUDA_VERSION\"] = \"117\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiwrpUr0cWyC"
   },
   "outputs": [],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmUBVEnvCDJv"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir='/content/unsloth-models'\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrrSMdlZlD6w"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    # instructions = examples[\"instruction\"]\n",
    "    # inputs       = examples[\"input\"]\n",
    "    # outputs      = examples[\"output\"]\n",
    "\n",
    "    # print(examples)\n",
    "\n",
    "    instructions = \"Answer the user's question accurately, thoroughly, and helpfully. Provide clear explanations with relevant details. If asked about medical or health-related topics, give informative responses while maintaining a balanced and educational tone.\"\n",
    "\n",
    "    inputs = examples['text'].split('<HUMAN>:')[-1].split('<ASSISTANT>:')[0].strip()\n",
    "    outputs = examples['text'].split('<HUMAN>:')[-1].split('<ASSISTANT>:')[1].strip()\n",
    "\n",
    "    # print(inputs)\n",
    "    texts = []\n",
    "    # for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "    text = alpaca_prompt.format(instructions, inputs, outputs) + EOS_TOKEN\n",
    "        # texts.append(text)\n",
    "    return { \"text\" : text, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"heliosbrahma/mental_health_chatbot_dataset\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O42K7szSlG3a"
   },
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "len(train_dataset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bZsfBuZDeCL"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wT1lpDwjcWyH"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# # from dotenv import load_dotenv\n",
    "# from huggingface_hub import login\n",
    "# import wandb\n",
    "\n",
    "# # load_dotenv()\n",
    "# # hf_token = os.environ['HF_TOKEN']\n",
    "# # wb_key = os.environ['WANDB_KEY']\n",
    "\n",
    "\n",
    "# # login(token=hf_token)\n",
    "# wandb.login()\n",
    "\n",
    "# # from datasets import load_dataset\n",
    "# # data = load_dataset(\"heliosbrahma/mental_health_chatbot_dataset\", split='train')\n",
    "# # data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xShtuTccWyI"
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# # to change the /cache/hugging_face download folder to custom path\n",
    "# !echo 'export HF_HOME=/DATA1/sayantan/hf_cache/' >> ~/.bashrc\n",
    "# !source ~/.bashrc\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpN2i9-zcWyK"
   },
   "outputs": [],
   "source": [
    "print(dataset['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvWNaHUcmpoh"
   },
   "outputs": [],
   "source": [
    "# dataset[0:100]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95_Nn-89DhsL"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-6,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = 'none', #\"wandb\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2ejIt2xSNKKp"
   },
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xTrAYZgcWyN"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Inspect the data types of normalization layers without changing them\n",
    "# for name, module in trainer.model.named_modules():\n",
    "#     if \"norm\" in name:\n",
    "#         print(f\"Layer: {name}, Data Type: {module.weight.dtype if hasattr(module, 'weight') else 'No weight parameter'}\")\n",
    "\n",
    "# trainer.model\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "# Assuming your model is already initialized as 'model'\n",
    "# Define input shape based on your model requirements\n",
    "# For a Mistral model, this would typically be [batch_size, sequence_length]\n",
    "batch_size = 1\n",
    "seq_length = 512\n",
    "\n",
    "# Generate the summary\n",
    "model_summary = summary(\n",
    "    trainer.model,  # Your PeftModelForCausalLM instance\n",
    "    input_data=torch.ones(batch_size, seq_length, dtype=torch.long),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    depth=10,  # Adjust depth to control how detailed the hierarchy is shown\n",
    "    device=next(model.parameters()).device,  # Use the same device as your model\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# print(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzU-z5CacWyP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqxqAZ7KJ4oL"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCqnaKmlO1U9"
   },
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n",
    "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxdGba7RcWyR"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kR3gIAX-SM2q"
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Answer the user's question accurately, thoroughly, and helpfully. Provide clear explanations with relevant details. If asked about medical or health-related topics, give informative responses while maintaining a balanced and educational tone.\", # instruction\n",
    "        \"How can I prevent anxiety and depression?\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
    "text = tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Pattern to match everything between \"### Response:\" and \"<|end_of_text|>\"\n",
    "pattern = r\"### Response:(.*?)(?=###|\\Z)\"\n",
    "\n",
    "# Extract with re.DOTALL to include newlines in the match\n",
    "match = re.search(pattern, text[0], re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    response_text = match.group(1).strip()\n",
    "    print(response_text)\n",
    "else:\n",
    "    print(\"No response found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WNmAdg4cWyS"
   },
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2pEuRb1r2Vg"
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Answer the user's question accurately, thoroughly, and helpfully. Provide clear explanations with relevant details. If asked about medical or health-related topics, give informative responses while maintaining a balanced and educational tone.\", # instruction\n",
    "        \"How can I prevent anxiety and depression?\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoqQ6Zj5cWyT"
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Answer the user's question accurately, thoroughly, and helpfully. Provide clear explanations with relevant details. If asked about medical or health-related topics, give informative responses while maintaining a balanced and educational tone. If the topic asked other than medical or health-related topics, do not provide any answer\", # instruction\n",
    "        \"How to overcome depression\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upcOlWe7A1vc"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"mistral-7b-v0.3\")  # Local saving\n",
    "tokenizer.save_pretrained(\"mistral-7b-v0.3\")\n",
    "# model.push_to_hub(\"sayantanBiswas/mistral-7b-v0.3\") # Online saving\n",
    "# tokenizer.push_to_hub(\"sayantanBiswas/mistral-7b-v0.3\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLjIIgb7lmBM"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KK0SQAq6jMB5"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "# message history part remains\n",
    "import re\n",
    "import os\n",
    "# is cuda version error persists, with incompatibility issue, try this !!\n",
    "# os.environ[\"BNB_CUDA_VERSION\"] = \"117\"\n",
    "# print(os.environ)\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "# Set environment variable to control model cache location (optional)\n",
    "# Uncomment and modify the path if you want to change the default cache location\n",
    "# os.environ[\"HF_HOME\"] = \"/path/to/your/model/cache\"\n",
    "\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"sayantanBiswas/mistral-7b-v0.3\"  # Change this to your preferred model\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "class ChatModel:\n",
    "    def __init__(self, model_id):\n",
    "        print(f\"Loading model {model_id} on {DEVICE} with {DTYPE}...\")\n",
    "\n",
    "        # Load the model with Unsloth\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_id,\n",
    "            max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dtype=DTYPE,\n",
    "            cache_dir='/content/unsloth-models'\n",
    "        )\n",
    "\n",
    "        # Configure generation parameters\n",
    "        self.generation_config = {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"do_sample\": True,\n",
    "            \"use_cache\": True,\n",
    "        }\n",
    "\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    def generate_response(self, history):\n",
    "        # Format the chat history for the model\n",
    "        # formatted_prompt = self.format_chat_history(history)\n",
    "        # print('model input: ', history)\n",
    "        original = ''\n",
    "        # Extract Response sections independently\n",
    "        response_pattern = r'### Response:\\s+(.*?)(?=###|\\Z)'\n",
    "        response_matches = re.findall(response_pattern, history, re.DOTALL)\n",
    "\n",
    "        # Clean and store response matches\n",
    "        if response_matches:\n",
    "            cleaned_responses = [match.strip() for match in response_matches if match.strip()]\n",
    "            if cleaned_responses:\n",
    "                original = cleaned_responses\n",
    "\n",
    "\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer([history], return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate without streaming\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                **self.generation_config\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        generated_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Return the full response\n",
    "        return original[0], generated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R38pLIublqIY"
   },
   "outputs": [],
   "source": [
    "\n",
    "chat = ChatModel(MODEL_ID)\n",
    "tokenizer = chat.tokenizer\n",
    "from tqdm import tqdm, trange\n",
    "val_data = []\n",
    "for i in trange(len(test_dataset)):\n",
    "\n",
    "  original, response = chat.generate_response(test_dataset[i]['text'])\n",
    "\n",
    "  # Extract only the response part using regex\n",
    "  pattern = r\"### Response:(.*?)(?=###|\\Z)\"\n",
    "  match = re.search(pattern, response, re.DOTALL)\n",
    "\n",
    "  if match:\n",
    "      clean_response = match.group(1).strip()\n",
    "  else:\n",
    "      clean_response = \"Failed to extract response\"\n",
    "\n",
    "  val_data.append((clean_response,original))\n",
    "\n",
    "  # print(response)\n",
    "  # print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA8PdHKFlsL2"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple whitespace + punctuation tokenizer.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text.lower().split()\n",
    "\n",
    "def calculate_bleu_simple(reference, candidate, max_n=4):\n",
    "    \"\"\"\n",
    "    Simplified BLEU score (no external dependencies).\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    for n in range(1, min(max_n + 1, len(candidate) + 1)):\n",
    "        candidate_ngrams = [tuple(candidate[i:i+n]) for i in range(len(candidate) - n + 1)]\n",
    "        candidate_counts = Counter(candidate_ngrams)\n",
    "\n",
    "        max_counts = Counter()\n",
    "        for ref in reference:\n",
    "            ref_ngrams = [tuple(ref[i:i+n]) for i in range(len(ref) - n + 1)]\n",
    "            ref_counts = Counter(ref_ngrams)\n",
    "            for ngram, count in ref_counts.items():\n",
    "                max_counts[ngram] = max(max_counts[ngram], count)\n",
    "\n",
    "        clipped = {ngram: min(count, max_counts[ngram]) for ngram, count in candidate_counts.items()}\n",
    "        numerator = sum(clipped.values())\n",
    "        denominator = sum(candidate_counts.values())\n",
    "        precision = numerator / denominator if denominator > 0 else 0\n",
    "        precisions.append(precision)\n",
    "\n",
    "    # Brevity penalty\n",
    "    ref_lens = [len(ref) for ref in reference]\n",
    "    closest_ref_len = min(ref_lens, key=lambda x: abs(x - len(candidate)))\n",
    "    bp = math.exp(1 - closest_ref_len / len(candidate)) if len(candidate) < closest_ref_len else 1.0\n",
    "\n",
    "    if all(p > 0 for p in precisions):\n",
    "        bleu = bp * math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
    "    else:\n",
    "        bleu = 0\n",
    "    return bleu\n",
    "\n",
    "def calculate_sacrebleu(reference, hypothesis):\n",
    "    return sacrebleu.sentence_bleu(hypothesis, [reference]).score\n",
    "\n",
    "def calculate_perplexity_4gram(text1, text2):\n",
    "    \"\"\"4-gram Laplace-smoothed perplexity between machine-generated (text1) and original (text2).\"\"\"\n",
    "    def tokenize(text):\n",
    "        text = text.replace('</s>', '')\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "\n",
    "    def build_ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "    tokens1 = tokenize(text1)\n",
    "    tokens2 = tokenize(text2)\n",
    "\n",
    "    fourgram_counts = Counter(build_ngrams(tokens2, 4))\n",
    "    trigram_counts = Counter(build_ngrams(tokens2, 3))\n",
    "\n",
    "    vocab = set(tokens1 + tokens2)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    test_fourgrams = build_ngrams(tokens1, 4)\n",
    "    if not test_fourgrams:\n",
    "        return float('inf')  # Prevent division by zero\n",
    "\n",
    "    log_sum = 0\n",
    "    for fg in test_fourgrams:\n",
    "        prefix = fg[:-1]\n",
    "        prob = (fourgram_counts.get(fg, 0) + 1) / (trigram_counts.get(prefix, 0) + vocab_size)\n",
    "        log_sum += math.log2(prob)\n",
    "\n",
    "    avg_log_prob = log_sum / len(test_fourgrams)\n",
    "    perplexity = math.pow(2, -avg_log_prob)\n",
    "    return perplexity\n",
    "\n",
    "def calculate_jaccard_similarity(text1, text2):\n",
    "    tokens1 = set(re.findall(r'\\w+|[^\\w\\s]', text1.lower()))\n",
    "    tokens2 = set(re.findall(r'\\w+|[^\\w\\s]', text2.lower()))\n",
    "    intersection = len(tokens1 & tokens2)\n",
    "    union = len(tokens1 | tokens2)\n",
    "    return intersection / union if union > 0 else 1.0\n",
    "\n",
    "def score_calculator(machine_generated, original):\n",
    "    machine_tokens = simple_tokenize(machine_generated)\n",
    "    original_tokens = simple_tokenize(original)\n",
    "    reference = [original_tokens]\n",
    "    candidate = machine_tokens\n",
    "\n",
    "    bleu_score = calculate_bleu_simple(reference, candidate)\n",
    "    sacre_score = calculate_sacrebleu(original, machine_generated)\n",
    "    perplexity_score = calculate_perplexity_4gram(machine_generated, original)\n",
    "    jaccard_score = calculate_jaccard_similarity(machine_generated, original)\n",
    "\n",
    "    return bleu_score, sacre_score, perplexity_score, jaccard_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOGfJE15l6lO"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for machine, original in tqdm(val_data):\n",
    "    bleu, sacre, perplexity, jaccard = score_calculator(machine, original)\n",
    "    scores.append((bleu, sacre, perplexity, jaccard))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOxdS7Agl9OH"
   },
   "outputs": [],
   "source": [
    "# scores\n",
    "import numpy as np\n",
    "np.mean(scores, axis = 0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llmv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
